<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Capture Image to Sine Wave Audio</title>
    <style>
        /* Style for the image preview */
        #imagePreview {
            width: 350px;
            height: 350px;
            margin-top: 20px;
        }
        #previewCanvas {
            display: none;
        }
    </style>
<body>
    <h1>Capture Image to Sine Wave Audio</h1>
    <button id="startButton" onclick="toggleCamera()">Start Camera</button>
	<button id="switchCameraButton" onclick="switchCamera()">Switch Camera</button>
	<p id="activeCamera">Current Camera: Back</p>
    <button id="captureButton" onclick="startCaptures()" disabled>Start Captures</button>
    <canvas id="previewCanvas" width="350" height="350"></canvas>
    <img id="imagePreview" src="" alt="Captured Image Preview" />
    <audio id="audioPlayer" controls></audio>
	<button id="decodeButton" onclick="decodeAudio()">Decode Audio to Image</button>
	<canvas id="decodedCanvas" width="350" height="350"></canvas>
	<p id="decodeMessage"></p>
    <script>
        let videoStream;
        let videoElement = document.createElement('video'); // Invisible video element
        let audioPlayer = document.getElementById('audioPlayer');
        let audioContext = new (window.AudioContext || window.webkitAudioContext)();
        let cameraOn = false;
        let currentCamera = 'environment'; // Default to back camera
        let captureInProgress = false;
        let captureInterval;
        // Toggle between front and back cameras
        function switchCamera() {
            currentCamera = currentCamera === 'environment' ? 'user' : 'environment'; // Switch camera
            if (cameraOn) {
                stopCamera();
                startCamera(); // Restart camera with new source
            }
        }
        // Start or stop the camera
        function toggleCamera() {
            if (cameraOn) {
                stopCamera();
                cameraOn = false;
                document.getElementById('startButton').innerText = 'Start Camera';
                document.getElementById('captureButton').disabled = true;
            } else {
                startCamera();
                document.getElementById('startButton').innerText = 'Stop Camera';
                document.getElementById('captureButton').disabled = false;
            }
        }	
		// Update the active camera label dynamically
		function updateCameraLabel() {
		    const cameraLabel = document.getElementById('activeCamera');
		    cameraLabel.innerText = `Current Camera: ${currentCamera === 'environment' ? 'Back' : 'Front'}`;
		}
		// Modify the switchCamera function to update the label
		function switchCamera() {
		    currentCamera = currentCamera === 'environment' ? 'user' : 'environment'; // Switch camera
		    updateCameraLabel(); // Update label after switching
		    if (cameraOn) {
		        stopCamera();
		        startCamera(); // Restart camera with the new source
		    }
		}
		// Initialize the label when the page loads
		updateCameraLabel();		
        // Start the camera with the current source (front/back)
        function startCamera() {
            const constraints = {
                video: { facingMode: { exact: currentCamera } }
            };
            navigator.mediaDevices.getUserMedia(constraints)
                .then(function (stream) {
                    videoStream = stream;
                    videoElement.srcObject = stream;
                    videoElement.play();
                    cameraOn = true;
                })
                .catch(function (err) {
                    console.error('Error accessing camera: ' + err);
                });
        }
        // Stop the camera
        function stopCamera() {
            videoStream.getTracks().forEach(track => track.stop());
        }
        // Start the automated capture process
        function startCaptures() {
            if (captureInProgress) return; // Prevent overlapping captures
            captureInProgress = true;
            // Trigger the first capture
            captureImage();
            // Set up an interval to repeat the process after the audio has played
            audioPlayer.onended = function() {
                if (captureInProgress) {
                    captureImage(); // Capture image after audio ends
                }
            };
        }
        // Capture the current image from the camera feed
        function captureImage() {
            if (captureInProgress) {
                // Capture image using the video feed
                const canvas = document.getElementById('previewCanvas');
                const ctx = canvas.getContext('2d');
                ctx.drawImage(videoElement, 0, 0, canvas.width, canvas.height); // Draw video frame onto canvas
                // Get image data
                const imageData = ctx.getImageData(0, 0, canvas.width, canvas.height);
                const imageURL = canvas.toDataURL(); // Convert canvas to image URL
                // Set the preview image source
                const previewImage = document.getElementById('imagePreview');
                previewImage.src = imageURL;
                // Convert the captured image to a sine wave audio
                const curveData = detectCurves(imageData);
                const grayscale = convertToGrayscale(curveData);
                // Generate sine wave audio from grayscale values
                generateAudio(grayscale);
            }
        }
        // Detect edges in the captured image using Sobel edge detection
        function detectCurves(imageData) {
            const width = imageData.width;
            const height = imageData.height;
            const data = imageData.data;
            const curveData = new ImageData(width, height);
            const curvePixels = curveData.data;
            for (let y = 1; y < height - 1; y++) {
                for (let x = 1; x < width - 1; x++) {
                    const i = (y * width + x) * 4;
                    // Sobel operator for edge detection
                    const gx = 
                        -data[i - 4 - width * 4] + data[i + 4 - width * 4] +
                        -2 * data[i - 4] + 2 * data[i + 4] +
                        -data[i - 4 + width * 4] + data[i + 4 + width * 4];
                    const gy = 
                        -data[i - 4 - width * 4] - 2 * data[i - width * 4] - data[i + 4 - width * 4] +
                        data[i - 4 + width * 4] + 2 * data[i + width * 4] + data[i + 4 + width * 4];
                    const magnitude = Math.sqrt(gx * gx + gy * gy);
                    // Threshold for edge detection
                    if (magnitude > 128) {
                        curvePixels[i] = 255; // Red
                        curvePixels[i + 1] = 255; // Green
                        curvePixels[i + 2] = 255; // Blue
                        curvePixels[i + 3] = 255; // Alpha
                    } else {
                        curvePixels[i + 3] = 0; // Transparent for non-edges
                    }
                }
            }
            return curveData;
        }
        // Convert detected edges to grayscale values
        function convertToGrayscale(imageData) {
            const pixels = imageData.data;
            const grayscale = [];
            for (let i = 0; i < pixels.length; i += 4) {
                const alpha = pixels[i + 3];
                if (alpha > 0) {
                    const gray = pixels[i] / 255;
                    grayscale.push(gray);
                } else {
                    grayscale.push(0); // Silence for non-curved regions
                }
            }
            return grayscale;
        }
		async function decodeAudio() {
		    const decodeMessage = document.getElementById('decodeMessage');
		    const decodedCanvas = document.getElementById('decodedCanvas');
		    const ctx = decodedCanvas.getContext('2d');

		    decodeMessage.innerText = 'Decoding audio...';

		    // Fetch the audio data from the audio player
		    const audioSrc = audioPlayer.src;
		    if (!audioSrc) {
		        decodeMessage.innerText = 'No audio to decode!';
		        return;
		    }

		    try {
		        const response = await fetch(audioSrc);
		        const audioData = await response.arrayBuffer();
		        const audioBuffer = await audioContext.decodeAudioData(audioData);

		        // Extract the audio data
		        const channelData = audioBuffer.getChannelData(0);
		        const totalSamples = channelData.length;
		        const grayscale = [];
		        const pixelsPerSample = Math.floor(totalSamples / (decodedCanvas.width * decodedCanvas.height));

		        // Decode grayscale values from the audio
		        for (let i = 0; i < channelData.length; i += pixelsPerSample) {
		            const average = channelData.slice(i, i + pixelsPerSample).reduce((a, b) => a + Math.abs(b), 0) / pixelsPerSample;
		            const grayValue = Math.min(255, Math.floor(average * 255));
		            grayscale.push(grayValue);
		        }

		        // Reconstruct the image on the canvas
		        const imageData = ctx.createImageData(decodedCanvas.width, decodedCanvas.height);
		        for (let i = 0; i < grayscale.length; i++) {
		            const grayValue = grayscale[i];
		            const idx = i * 4;
		            imageData.data[idx] = grayValue;     // Red
		            imageData.data[idx + 1] = grayValue; // Green
		            imageData.data[idx + 2] = grayValue; // Blue
		            imageData.data[idx + 3] = 255;       // Alpha
		        }
		        ctx.putImageData(imageData, 0, 0);

		        decodeMessage.innerText = 'Decoding complete! Image reconstructed.';
		    } catch (error) {
		        console.error('Error decoding audio:', error);
		        decodeMessage.innerText = 'Error decoding audio.';
		    }
		}
        // Generate audio from grayscale values
		function generateAudio(grayscale) {
		    const sampleRate = audioContext.sampleRate; // Sample rate of the audio
		    const totalSamples = grayscale.length; // One sample per pixel
		    const frequencyBase = 20; // Base frequency for the sine wave
		    const frequencyRange = 20000 - frequencyBase; // Frequency range for grayscale mapping
		    const buffer = audioContext.createBuffer(1, totalSamples, sampleRate); // Create audio buffer
		    const channel = buffer.getChannelData(0); // Get the channel data array

		    // Generate audio samples based on grayscale values
		    for (let i = 0; i < grayscale.length; i++) {
		        const pixelValue = grayscale[i]; // Grayscale value (normalized 0 to 1)
		        const frequency = frequencyBase + pixelValue * frequencyRange; // Map pixel to frequency
		        const amplitude = pixelValue; // Amplitude is proportional to grayscale value
		        const t = i / sampleRate; // Time for the current sample
		        channel[i] = amplitude * Math.sin(2 * Math.PI * frequency * t); // Generate sine wave
		    }

		    // Normalize the audio signal
		    const maxAmplitude = Math.max(...channel.map(Math.abs));
		    if (maxAmplitude > 0) {
		        for (let i = 0; i < channel.length; i++) {
		            channel[i] /= maxAmplitude;
		        }
		    }

		    // Export the audio as WAV and play it
		    const wavBlob = bufferToWave(buffer, totalSamples); // Convert buffer to WAV
		    audioPlayer.src = URL.createObjectURL(wavBlob); // Set audio source
		    if (audioPlayer.paused) {
		        audioPlayer.play().catch((err) => {
		            console.error('Audio play error:', err);
		        });
		    }
		}
        // Convert AudioBuffer to WAV format
        function bufferToWave(buffer, totalSamples) {
            const wavArray = new Uint8Array(44 + totalSamples * 2);
            const view = new DataView(wavArray.buffer);
            // RIFF header
            writeString(view, 0, 'RIFF');
            view.setUint32(4, 36 + totalSamples * 2, true);
            writeString(view, 8, 'WAVE');
            writeString(view, 12, 'fmt ');
            view.setUint32(16, 16, true);
            view.setUint16(20, 1, true);
            view.setUint16(22, 1, true);
            view.setUint32(24, audioContext.sampleRate, true);
            view.setUint32(28, audioContext.sampleRate * 2, true);
            view.setUint16(32, 2, true);
            view.setUint16(34, 16, true);
            writeString(view, 36, 'data');
            view.setUint32(40, totalSamples * 2, true);
            // Audio data
            let offset = 44;
            const channelData = buffer.getChannelData(0);
            for (let i = 0; i < totalSamples; i++) {
                view.setInt16(offset, channelData[i] * 32767, true);
                offset += 2;
            }
            return new Blob([wavArray], { type: 'audio/wav' });
        }
        // Write a string into the WAV header
        function writeString(view, offset, string) {
            for (let i = 0; i < string.length; i++) {
                view.setUint8(offset + i, string.charCodeAt(i));
            }
        }		
		const playAudio = () => {
		    audioPlayer.src = URL.createObjectURL(wavBlob);
		    audioPlayer.play().catch((err) => {
		        console.error('Audio play error:', err);
		    });
		};

		document.getElementById('captureButton').addEventListener('click', () => {
		    startCaptures();
		});
    </script>
</body>
</html>
