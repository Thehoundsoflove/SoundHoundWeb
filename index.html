<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Capture Image to Sine Wave Audio</title>
    <style>
        /* Style for the image preview */
        #imagePreview {
            width: 350px;
            height: 350px;
            margin-top: 20px;
        }
        #previewCanvas {
            display: none;
        }
    </style>
<body>
    <h1>Capture Image to Sine Wave Audio</h1>
    <label for="resolutionSlider">Resolution: </label>
    <input type="range" id="resolutionSlider" min="50" max="350" value="150" step="50">
    <span id="resolutionValue">150x150</span>    <button id="startButton" onclick="toggleCamera()">Start Camera</button>
	<label for="edgeThresholdSlider">Edge Threshold: </label>
	<input type="range" id="edgeThresholdSlider" min="60" max="100" value="72" step="1">
	<span id="edgeThresholdValue">72</span>
	<button id="switchCameraButton" onclick="switchCamera()">Switch Camera</button>
	<p id="activeCamera">Current Camera: Back</p>
    <button id="captureButton" onclick="startCaptures()" disabled>Start Captures</button>
    <canvas id="previewCanvas" width="350" height="350"></canvas>
    <img id="imagePreview" src="" alt="Captured Image Preview" />
    <audio id="audioPlayer" controls></audio>
	<button id="decodeButton" onclick="decodeAudio()">Decode Audio to Image</button>
	<canvas id="decodedCanvas" width="350" height="350"></canvas>
	<p id="decodeMessage"></p>
    <script>
		// Get the slider and the span that displays the threshold value
		const edgeThresholdSlider = document.getElementById('edgeThresholdSlider');
		const edgeThresholdValue = document.getElementById('edgeThresholdValue');

		// Initialize the edgeThreshold value
		let edgeThreshold = 55;

		// Function to update the edgeThreshold and the displayed value
		function updateEdgeThreshold() {
		    edgeThreshold = edgeThresholdSlider.value;
		    edgeThresholdValue.innerText = edgeThreshold; // Display the current value
		}

		// Listen for changes to the slider
		edgeThresholdSlider.addEventListener('input', updateEdgeThreshold);

		// Initialize the display with the initial threshold value
		updateEdgeThreshold();
		const resolutionSlider = document.getElementById('resolutionSlider');
		const resolutionValue = document.getElementById('resolutionValue');
        let videoStream;
        let videoElement = document.createElement('video'); // Invisible video element
        let audioPlayer = document.getElementById('audioPlayer');
        let audioContext = new (window.AudioContext || window.webkitAudioContext)();
        let cameraOn = false;
        let currentCamera = 'environment'; // Default to back camera
        let captureInProgress = false;
        let captureInterval;
		
		function updateResolution() {
		    // Map the slider value to a resolution
		    const resolution = resolutionSlider.value;
		    const previewCanvas = document.getElementById('previewCanvas');
		    const decodedCanvas = document.getElementById('decodedCanvas');
    
		    // Update canvas dimensions
		    previewCanvas.width = resolution;
		    previewCanvas.height = resolution;
		    decodedCanvas.width = resolution;
		    decodedCanvas.height = resolution;

		    // Update the displayed resolution value
		    resolutionValue.innerText = `${resolution}x${resolution}`;
		}

		// Initialize canvas with the default resolution (350x350)
		updateResolution();

		// Listen for changes to the slider
		resolutionSlider.addEventListener('input', updateResolution);
		// Toggle between front and back cameras
        function switchCamera() {
            currentCamera = currentCamera === 'environment' ? 'user' : 'environment'; // Switch camera
            if (cameraOn) {
                stopCamera();
                startCamera(); // Restart camera with new source
            }
        }
        // Start or stop the camera
        function toggleCamera() {
            if (cameraOn) {
                stopCamera();
                cameraOn = false;
                document.getElementById('startButton').innerText = 'Start Camera';
                document.getElementById('captureButton').disabled = true;
            } else {
                startCamera();
                document.getElementById('startButton').innerText = 'Stop Camera';
                document.getElementById('captureButton').disabled = false;
            }
        }	
		// Update the active camera label dynamically
		function updateCameraLabel() {
		    const cameraLabel = document.getElementById('activeCamera');
		    cameraLabel.innerText = `Current Camera: ${currentCamera === 'environment' ? 'Back' : 'Front'}`;
		}
		// Modify the switchCamera function to update the label
		function switchCamera() {
		    currentCamera = currentCamera === 'environment' ? 'user' : 'environment'; // Switch camera
		    updateCameraLabel(); // Update label after switching
		    if (cameraOn) {
		        stopCamera();
		        startCamera(); // Restart camera with the new source
		    }
		}
		// Initialize the label when the page loads
		updateCameraLabel();		
        // Start the camera with the current source (front/back)
        function startCamera() {
            const constraints = {
                video: { facingMode: { exact: currentCamera } }
            };
            navigator.mediaDevices.getUserMedia(constraints)
                .then(function (stream) {
                    videoStream = stream;
                    videoElement.srcObject = stream;
                    videoElement.play();
                    cameraOn = true;
                })
                .catch(function (err) {
                    console.error('Error accessing camera: ' + err);
                });
        }
        // Stop the camera
        function stopCamera() {
            videoStream.getTracks().forEach(track => track.stop());
        }
        // Start the automated capture process
        function startCaptures() {
            if (captureInProgress) return; // Prevent overlapping captures
            captureInProgress = true;
            // Trigger the first capture
            captureImage();
            // Set up an interval to repeat the process after the audio has played
            audioPlayer.onended = function() {
                if (captureInProgress) {
                    captureImage(); // Capture image after audio ends
                }
            };
        }
        // Capture the current image from the camera feed
		function captureImage() {
		    if (captureInProgress) {
		        // Capture image using the video feed
		        const canvas = document.getElementById('previewCanvas');
		        const ctx = canvas.getContext('2d');
		        ctx.drawImage(videoElement, 0, 0, canvas.width, canvas.height); // Draw video frame onto canvas

		        // Get image data
		        const imageData = ctx.getImageData(0, 0, canvas.width, canvas.height);
		        const imageURL = canvas.toDataURL(); // Convert canvas to image URL

		        // Set the preview image source
		        const previewImage = document.getElementById('imagePreview');
		        previewImage.src = imageURL;

		        // Convert the captured image to grayscale and calculate depth
		        const { grayscale, depth } = convertToGrayscaleAndDepth(imageData);

		        // Visualize the depth data
		        visualizeDepth(depth);

		        // Generate sine wave audio from grayscale values
		        generateAudio(grayscale);
		    }
		}
		
		function visualizeDepth(depth) {
		    const decodedCanvas = document.getElementById('decodedCanvas');
		    const ctx = decodedCanvas.getContext('2d');
		    const imageData = ctx.createImageData(decodedCanvas.width, decodedCanvas.height);

		    for (let i = 0; i < depth.length; i++) {
		        const depthValue = depth[i];

		        // Map depth value to grayscale (0 to 255)
		        const grayValue = Math.floor(depthValue * 255); // Map depth to 0-255

		        // Set the pixel color based on depth
		        const idx = i * 4;
		        imageData.data[idx] = grayValue;     // Red
		        imageData.data[idx + 1] = grayValue; // Green
		        imageData.data[idx + 2] = grayValue; // Blue
		        imageData.data[idx + 3] = 255;       // Alpha (fully opaque)
		    }

		}
		

        // Detect edges in the captured image using Sobel edge detection
		function detectCurves(imageData) {
		    const width = imageData.width;
		    const height = imageData.height;
		    const data = imageData.data;
		    const curveData = new ImageData(width, height);
		    const curvePixels = curveData.data;

		    // Use the dynamic edgeThreshold value from the slider
		    const threshold = edgeThreshold;

		    // Array to keep track of visited pixels
		    const visited = Array(width * height).fill(false);
		    let label = 0;
		    const regions = [];

		    for (let y = 1; y < height - 1; y++) {
		        for (let x = 1; x < width - 1; x++) {
		            const i = (y * width + x) * 4;
		            const gx =
		                -1 * data[i - 4 - width * 4] + 1 * data[i + 4 - width * 4] +
		                -2 * data[i - 4] + 2 * data[i + 4] +
		                -1 * data[i - 4 + width * 4] + 1 * data[i + 4 + width * 4];
		            const gy =
		                -1 * data[i - 4 - width * 4] - 2 * data[i - width * 4] - 1 * data[i + 4 - width * 4] +
		                1 * data[i - 4 + width * 4] + 2 * data[i + width * 4] + 1 * data[i + 4 + width * 4];
		            const magnitude = Math.sqrt(gx * gx + gy * gy);

		            // Apply the threshold to detect edges
		            if (magnitude > threshold) {
		                curvePixels[i] = 255;     // Red
		                curvePixels[i + 1] = 255; // Green
		                curvePixels[i + 2] = 255; // Blue
		                curvePixels[i + 3] = 255; // Alpha (fully opaque)

		                // If the pixel is an edge and not visited, start a region detection
		                if (!visited[y * width + x]) {
		                    labelRegion(x, y, label++, width, height, visited, curvePixels, data, threshold);
		                }
		            } else {
		                curvePixels[i + 3] = 0; // Transparency for non-edges
		            }
		        }
		    }

		    return curveData;
		}

		// Region labeling: Marks connected pixels as part of a region
		function labelRegion(x, y, label, width, height, visited, curvePixels, data, threshold) {
		    const queue = [[x, y]]; // BFS queue for region growing
		    visited[y * width + x] = true;

		    while (queue.length > 0) {
		        const [curX, curY] = queue.shift();
		        const idx = (curY * width + curX) * 4;

		        // Mark the region label in the image
		        curvePixels[idx] = label * 30; // Color the region based on the label
		        curvePixels[idx + 1] = label * 30; // Use the label as the RGB component
		        curvePixels[idx + 2] = label * 30;

		        // Check 8-connected neighbors
		        for (let dy = -1; dy <= 1; dy++) {
		            for (let dx = -1; dx <= 1; dx++) {
		                const newX = curX + dx;
		                const newY = curY + dy;
		                if (newX >= 0 && newX < width && newY >= 0 && newY < height) {
		                    const newIdx = (newY * width + newX) * 4;
		                    if (!visited[newY * width + newX] && Math.abs(data[newIdx] - data[idx]) > threshold) {
		                        visited[newY * width + newX] = true;
		                        queue.push([newX, newY]);
		                    }
		                }
		            }
		        }
		    }
		}

		// Convert detected edges to grayscale values and calculate depth
		function convertToGrayscaleAndDepth(imageData) {
		    const pixels = imageData.data;
		    const grayscale = [];
		    const depth = [];

		    for (let i = 0; i < pixels.length; i += 4) {
		        const alpha = pixels[i + 3];
		        if (alpha > 0) {
		            const gray = pixels[i] / 255; // Normalize to 0-1 range for grayscale
		            grayscale.push(gray);

		            // Estimate depth based on the grayscale value (inverse relation: darker = farther)
		            const depthValue = 1 - gray; // You can tweak this depending on how you want depth to behave
		            depth.push(depthValue);
		        } else {
		            grayscale.push(0); // Silence for non-curved regions
		            depth.push(0); // No depth for non-curved regions
		        }
		    }

		    return { grayscale, depth };
		}


		async function decodeAudio() {
		    const decodeMessage = document.getElementById('decodeMessage');
		    const decodedCanvas = document.getElementById('decodedCanvas');
		    const ctx = decodedCanvas.getContext('2d');

		    decodeMessage.innerText = 'Decoding audio...';

		    // Fetch the audio data from the audio player
		    const audioSrc = audioPlayer.src;
		    if (!audioSrc) {
		        decodeMessage.innerText = 'No audio to decode!';
		        return;
		    }

		    try {
		        const response = await fetch(audioSrc);
		        const audioData = await response.arrayBuffer();
		        const audioBuffer = await audioContext.decodeAudioData(audioData);

		        // Extract the audio data
		        const channelData = audioBuffer.getChannelData(0);
		        const totalSamples = channelData.length;
		        const grayscale = [];
		        const pixelsPerSample = Math.floor(totalSamples / (decodedCanvas.width * decodedCanvas.height));

		        // Decode grayscale values from the audio
		        for (let i = 0; i < channelData.length; i += pixelsPerSample) {
		            const average = channelData.slice(i, i + pixelsPerSample).reduce((a, b) => a + Math.abs(b), 0) / pixelsPerSample;
		            const grayValue = Math.min(255, Math.floor(average * 255));
		            grayscale.push(grayValue);
		        }

		        // Reconstruct the image on the canvas
		        const imageData = ctx.createImageData(decodedCanvas.width, decodedCanvas.height);
		        for (let i = 0; i < grayscale.length; i++) {
		            const grayValue = grayscale[i];
		            const idx = i * 4;
		            imageData.data[idx] = grayValue;     // Red
		            imageData.data[idx + 1] = grayValue; // Green
		            imageData.data[idx + 2] = grayValue; // Blue
		            imageData.data[idx + 3] = 255;       // Alpha
		        }
		        ctx.putImageData(imageData, 0, 0);

		        decodeMessage.innerText = 'Decoding complete! Image reconstructed.';
		    } catch (error) {
		        console.error('Error decoding audio:', error);
		        decodeMessage.innerText = 'Error decoding audio.';
		    }
		}

		// Modify the generateAudio function to handle different waveforms
		function generateAudio(grayscale) {
		    const sampleRate = audioContext.sampleRate;
		    const totalSamples = grayscale.length;
		    const frequencyBase = 500; // Start frequency (in Hz)
		    const frequencyRange = 3000; // Maximum frequency for mapping
		    const buffer = audioContext.createBuffer(1, totalSamples, sampleRate);
		    const channel = buffer.getChannelData(0);

		    // Generate audio samples based on grayscale values
		    for (let i = 0; i < grayscale.length; i++) {
		        const pixelValue = grayscale[i]; // Grayscale value (0 to 1)
		        const frequency = frequencyBase + pixelValue * frequencyRange; // Map pixel value to frequency
		        const amplitude = pixelValue; // Amplitude is proportional to grayscale value
		        const t = i / sampleRate; // Time for the current sample

		        let waveSample;

		        // Threshold frequency to distinguish between high and low frequencies
		        const pulseThreshold = 1500; // Frequency above which we use a beep

		        if (frequency >= pulseThreshold) {
		            // For high frequencies, create sharp pulses (beeps)
		            const pulseDuration = 0.005; // Duration of the pulse in seconds
		            if (t % (1 / frequency) < pulseDuration) {
		                waveSample = amplitude; // Sharp pulse for high frequencies (beep)
		            } else {
		                waveSample = 0; // No sound in between pulses
		            }
		        } else {
		            // For low frequencies, create sharp clicks (low frequency pulses)
		            const pulseDuration = 0.01; // Duration of the pulse in seconds
		            if (t % (1 / frequency) < pulseDuration) {
		                waveSample = amplitude; // Sharp pulse for low frequencies (click)
		            } else {
		                waveSample = 0; // No sound in between pulses
		            }
		        }

		        channel[i] = waveSample; // Set the sample value in the channel
		    }

		    // Normalize the audio signal
		    const maxAmplitude = Math.max(...channel.map(Math.abs));
		    if (maxAmplitude > 0) {
		        for (let i = 0; i < channel.length; i++) {
		            channel[i] /= maxAmplitude;
		        }
		    }

		    // Export the audio as WAV and play it
		    const wavBlob = bufferToWave(buffer, totalSamples);
		    audioPlayer.src = URL.createObjectURL(wavBlob);
		    if (audioPlayer.paused) {
		        audioPlayer.play().catch((err) => {
		            console.error('Audio play error:', err);
		        });
		    }
		}


        // Convert AudioBuffer to WAV format
        function bufferToWave(buffer, totalSamples) {
            const wavArray = new Uint8Array(44 + totalSamples * 2);
            const view = new DataView(wavArray.buffer);
            // RIFF header
            writeString(view, 0, 'RIFF');
            view.setUint32(4, 36 + totalSamples * 2, true);
            writeString(view, 8, 'WAVE');
            writeString(view, 12, 'fmt ');
            view.setUint32(16, 16, true);
            view.setUint16(20, 1, true);
            view.setUint16(22, 1, true);
            view.setUint32(24, audioContext.sampleRate, true);
            view.setUint32(28, audioContext.sampleRate * 2, true);
            view.setUint16(32, 2, true);
            view.setUint16(34, 16, true);
            writeString(view, 36, 'data');
            view.setUint32(40, totalSamples * 2, true);
            // Audio data
            let offset = 44;
            const channelData = buffer.getChannelData(0);
            for (let i = 0; i < totalSamples; i++) {
                view.setInt16(offset, channelData[i] * 32767, true);
                offset += 2;
            }
            return new Blob([wavArray], { type: 'audio/wav' });
        }
        // Write a string into the WAV header
        function writeString(view, offset, string) {
            for (let i = 0; i < string.length; i++) {
                view.setUint8(offset + i, string.charCodeAt(i));
            }
        }		
		const playAudio = () => {
		    audioPlayer.src = URL.createObjectURL(wavBlob);
		    audioPlayer.play().catch((err) => {
		        console.error('Audio play error:', err);
		    });
		};

		document.getElementById('captureButton').addEventListener('click', () => {
		    startCaptures();
		});
    </script>
</body>
</html>